{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNRKzP2P5jPgYnCYmDNZqGR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jfodera/proj-ai-ml/blob/main/homeworks/hw1/hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1 -  Derive the objective function for Logistic Regression using Maximum Likelihood Estimation (MLE)\n",
        "\n",
        "## Background\n",
        "### Logistic Regression\n",
        "- Using linear regression to predict binary classification will result in a continuous outcome.\n",
        "- A problem with bounded target outcomes (specifically binary classification) will use logistic regression.\n",
        "- In Logistic Regression, the model doesn't output \"0\" or \"1\" directly. It outputs a continuous value between 0 and 1, which we interpret as the probability that the input $x_i$ belongs to the \"positive\" class ($y=1$).\n",
        "- To do this, we use an activation on top of our linear model(from linear regression)\n",
        "  - usually the sigmoid function.\n",
        "  - 'activation' is simply a function applied to the output of a linear model.\n",
        "- The predicted outcome:\n",
        "  - $\\hat{y}$= $\\sigma(\\textbf{w}^Tx + b)$\n",
        "  - where $\\sigma(z) = \\frac{1}{(1+e^{-z})}$\n",
        "\n",
        "### The Likelihood Function\n",
        "- in Logistic RegressionIn Logistic Regression, our \"outcome\" is a series of $0$s and $1$s (e.g., \"Not Spam\" or \"Spam\").\n",
        "\n",
        "Our \"rules\" are the weights ($\\beta$).For every data point $i$:The model calculates a probability $p_i$ using the sigmoid function.If the actual label is $y=1$, the likelihood for that point is simply $p_i$.If the actual label is $y=0$, the likelihood for that point is $(1 - p_i)$.The Joint LikelihoodThe Likelihood Function $L(\\beta)$ is the product of all these individual probabilities. If you have three data points where the first two are $1$ and the last is $0$, the likelihood is:$$L(\\beta) = p_1 \\cdot p_2 \\cdot (1 - p_3)$$If the model is \"good,\" all these numbers will be close to $1$, making the total product high. If the model is \"bad\" and predicts a $0.1$ for a $y=1$ case, the entire product collapses toward zero[[1]](https://czep.net/stat/mlelr.pdf).\n",
        "\n",
        "### MLE (Maximum Likelihood Estimation)\n",
        "- *likelihood function* - The likelihood function measures the probability of observing the given data under the assumed model.\n",
        "  - think of the Likelihood Function as a \"Summary of Success\" for your model's current settings.\n",
        "  - we want to maximize this\n",
        "- Most ML models are optimization problems focused on minimization of error.\n",
        "- MLE is a recipe for formulating the loss function that is to be minimized\n",
        "  - MLE minimizes the loss function\n",
        "- MLE is meant to find parameter values within the training data that maximize the likelihood function\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Misc Definitions\n",
        "- **objective function** - what we are trying to achieve\n",
        "- **parameters** - $\\theta$ or β or weights aka the parameters that are the weights the model learns\n",
        "  - these are variable and what we are trying to optimize to create a model that best fits the data\n",
        "- **Features** - inputs that are fixed\n",
        "  - e.g. square footage of house of persons age\n",
        "- **Loss function** - is a way to quantify difference between the real and predicted value of the target\n",
        "- **Cost Function** - Loss function across entire set of data\n",
        "- **NLL** - We use the negative log-likelihood so that probabilistic model fitting fits neatly into the loss-minimization framework used in machine learning.\n",
        "- **follow a Bernoulli distribution** - Just another way of saying its a simple binary classification problem.\n",
        "  - Typical bernoulli: $$f(k; p) = p^k (1 - p)^{1 - k} \\quad \\text{for } k \\in \\{0, 1\\}$$\n",
        "\n",
        "### Citations\n",
        "[1]Czepiel, Scott A. Maximum Likelihood Estimation of Logistic Regression Models: Theory and Implementation. Carnegie Mellon University, 2022. PDF, https://czep.net/stat/mlelr.pdf\n",
        "\n"
      ],
      "metadata": {
        "id": "p9fto-Hi-IoS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1 Answer\n",
        "\n",
        "## In Words\n",
        "The objective function of logistic regression is empoweer a model to efficiently tackle problems with bounded target outcomes (specifically binary classifications). A logistic regression model does not output \"0\" or \"1\" directly. Rather, a continuous value between 0 and 1, which we interpret as the probability that the input $x_i$ belongs to the \"positive\" class ($y=1$).\n",
        "\n",
        "Maximum Likelihood Estimation (MLE) is the part of logistic regression that helps to optimize the parameters. It finds the best parameters by attempting to maximize the likelihood function. But overall, is a recipe for formulating the loss function that is minimized.\n",
        "\n",
        "## Mathmatical Derivation\n",
        "\n",
        "### 1. Start with Bernoulli\n",
        "\n",
        "Since we are dealing with binary classification (0 or 1), we assume the labels follow a Bernoulli distribution. The probability of a single observation is:\n",
        "\n",
        "- $P(y^{(i)} | x^{(i)}) = (\\hat{y}^{(i)})^{y^{(i)}} (1 - \\hat{y}^{(i)})^{(1 - y^{(i)})}$\n",
        "\n",
        "\n",
        "### 2. The Likelihood Function\n",
        "\n",
        "We want to maximize the probability of seeing all our actual labels. This is maximizing the likelihood funciton which measures the probability of observing the given data under the assumed model. Assuming the data points are independent, we multiply their individual probabilities:\n",
        "\n",
        "- $L(\\theta) = \\prod_{i=1}^{n} (\\hat{y}^{(i)})^{y^{(i)}} (1 - \\hat{y}^{(i)})^{(1 - y^{(i)})}$\n",
        "  - 'L' stands for likelihood\n",
        "  - '$(\\theta)$' represents the parameters (weights and bias) of the model\n",
        "    - shows likelihood is a function of models internal settings\n",
        "  - $\\prod$ - like a summation but instead you multiply everything together ]\n",
        "  - 'n' - total number of oberservations\n",
        "  - Calulate the independant probability for every data point and multiply them all together\n",
        "\n",
        "\n",
        "\n",
        "### 3. Log-Likelihood Transformation\n",
        "\n",
        "Multiplying many decimals (probabilities) leads to \"arithmetic underflow\" (numbers becoming too small for computers to handle). We apply a **natural log** to each side, turning the products into sums and countering this error :\n",
        "\n",
        "- $L(\\theta) = \\prod_{i=1}^{n} P(y^{(i)} | x^{(i)})$\n",
        "  - log turns product $\\prod$ into $\\sum$\n",
        "- $\\log L(\\theta) = \\sum_{i=1}^{n} \\log \\left( (\\hat{y}^{(i)})^{y^{(i)}} (1 - \\hat{y}^{(i)})^{(1 - y^{(i)})} \\right)$\n",
        "  - takign the log of a product turns it to a sum: $\\log(A \\cdot B) = \\log(A) + \\log(B)$.\n",
        "- $\\log L(\\theta) = \\sum_{i=1}^{n} (\\log((\\hat{y}^{(i)})^{y^{(i)}}) + \\log((1 - \\hat{y}^{(i)})^{(1 - y^{(i)})}))$\n",
        "  - use the power rule: $\\log(a^b) = b \\log a$\n",
        "\n",
        "- $\\log L(\\theta) = \\sum_{i=1}^{n} [y^{(i)} \\log \\hat{y}^{(i)} + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)})]$\n",
        "\n",
        "### 4. Defining the Loss and Cost Functions (Objective Function)\n",
        "\n",
        "We want to prefer to **minimize** a \"loss\" rather than maximize a \"likelihood.\" So we negating the log-likelihood for each induvidual point and averaging it over the whole dataset.\n",
        "\n",
        "- **Loss Function** for a single instance:\n",
        "  - $l^{i}(y^{(i)},\\hat{y}^{(i)}) = -(y^{(i)} \\log \\hat{y}^{(i)} + (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}))$\n",
        "\n",
        "\n",
        "- **Cost Function** for the entire dataset:\n",
        "  - $L(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} l^{i}(y^{(i)}, \\hat{y}^{(i)})$\n",
        "\n"
      ],
      "metadata": {
        "id": "dxVwfxCpeODh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1 - MAP technique for Logistic Regression\n",
        "\n",
        "## Background\n",
        "### Misc Definitions\n",
        "- **Bayesian Approach** - way of doing statistics where you don't just look at the data in front of you, but also factor in what you already know\n",
        "\n",
        "# BELOW IS UNFINISHED/UNEDITED\n",
        "\n",
        "**Maximum A Posteriori (MAP) estimation** is a Bayesian approach used to estimate the parameters of logistic regression (or other models). It finds the parameter values that maximize the **posterior probability** of the parameters given the observed data.\n",
        "\n",
        "In logistic regression, the standard **objective function** derived from **Maximum Likelihood Estimation (MLE)** maximizes the likelihood of observing the binary labels given the features and parameters (weights **w**). This is equivalent to minimizing the negative log-likelihood, often written as:\n",
        "\n",
        "- **MLE objective**: Minimize ∑ log(1 + exp(-yᵢ wᵀ xᵢ)) (for appropriately signed labels yᵢ ∈ {±1} or adjusted for {0,1}).\n",
        "\n",
        "**MAP** incorporates prior beliefs about the parameters via Bayes' theorem:\n",
        "\n",
        "Posterior ∝ Likelihood × Prior\n",
        "\n",
        "Thus, the MAP estimate maximizes:\n",
        "\n",
        "log Posterior = log Likelihood + log Prior + constant\n",
        "\n",
        "Or equivalently, minimizes the negative log posterior:\n",
        "\n",
        "- **MAP objective**: Negative log-likelihood + negative log-prior (which acts as a penalty/regularization term).\n",
        "\n",
        "### Key Differences Between MAP and MLE in Logistic Regression\n",
        "\n",
        "- **MLE** is a **frequentist** method: It only uses the observed data (likelihood) to find the parameters that make the data most probable. It assumes no prior knowledge about the parameters and can lead to overfitting, especially with limited data or high-dimensional features, as it may produce large parameter values.\n",
        "\n",
        "- **MAP** is a **Bayesian** (point-estimate) method: It combines the likelihood with a **prior distribution** over the parameters. This incorporates prior beliefs (e.g., parameters should be small or sparse), making MAP a form of **regularized** estimation. MAP provides a single point estimate (the mode of the posterior), not the full posterior distribution (unlike full Bayesian inference).\n",
        "\n",
        "A common practical connection: Many regularized logistic regression formulations are equivalent to MAP with specific priors.\n",
        "\n",
        "- **L2 regularization** (Ridge) → MAP with a **Gaussian prior** centered at 0 (weights assumed small, normally distributed). The penalty term is λ ||w||², which comes from -log(prior).\n",
        "\n",
        "- **L1 regularization** (Lasso) → MAP with a **Laplace prior** (promotes sparsity, i.e., some weights exactly zero).\n",
        "\n",
        "This is why adding regularization to logistic regression (common in libraries like scikit-learn) can be interpreted as performing MAP estimation.\n",
        "\n",
        "When the prior is uniform (flat, improper), MAP reduces exactly to MLE, as the prior contributes no additional information.\n",
        "\n",
        "MAP helps prevent overfitting by shrinking parameters toward the prior (especially useful with small datasets), while MLE relies purely on data and may overfit.\n",
        "\n",
        "### Citations and Sources\n",
        "\n",
        "- Wikipedia on Maximum a posteriori estimation explains that \"MAP estimation is therefore a regularization of maximum likelihood estimation\" and discusses its relation to priors.\n",
        "\n",
        "- Machine Learning Mastery provides a gentle introduction, noting that MAP gives a Bayesian foundation for algorithms like logistic regression and explicitly allows prior beliefs, unlike MLE.\n",
        "\n",
        "- Cornell CS4780 lecture notes on Logistic Regression describe MAP as treating weights as random variables with a prior, leading to regularized estimation (e.g., via Gaussian prior for L2).\n",
        "\n",
        "- Agustinus Kristiadi's blog post derives the mathematical connection, showing that MAP = MLE + prior term (weighted likelihood).\n",
        "\n",
        "- Wikipedia's Logistic regression page states: \"The use of a regularization condition is equivalent to doing maximum a posteriori (MAP) estimation, an extension of maximum likelihood.\"\n",
        "\n",
        "In practice, when you implement regularized logistic regression (e.g., with scikit-learn's `LogisticRegression` using `penalty='l2'`), you're effectively performing MAP estimation under a Gaussian prior—even if the code frames it as penalized MLE. This bridges frequentist regularization and Bayesian reasoning."
      ],
      "metadata": {
        "id": "QJ56JEt4gGWf"
      }
    }
  ]
}